{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loong/miniconda3/envs/Green_Halo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 16 files: 100%|██████████| 16/16 [00:00<00:00, 27719.48it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.43s/it]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "AWQ:   0%|          | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
      "AWQ: 100%|██████████| 32/32 [17:03<00:00, 31.98s/it]\n",
      "Note that `shard_checkpoint` is deprecated and will be removed in v4.44. We recommend you using split_torch_state_dict_into_shards from huggingface_hub library\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with w_bit=8 is quantized and saved at \"/mnt/2T/Codes/models/quantized_model/Llama-3.1-8B-Instruct-AWQ-8bit\", time: 1036.83 seconds\n"
     ]
    }
   ],
   "source": [
    "import time, os, torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "save_dir = \"/mnt/2T/Codes/models/quantized_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_path = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# List of w_bit values\n",
    "w_bits = [8]\n",
    "for w_bit in w_bits:\n",
    "    quant_config[\"w_bit\"] = w_bit\n",
    "    # Load model\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        model_path\n",
    "        , trust_remote_code=True\n",
    "        , device_map=\"auto\"\n",
    "        # , low_cpu_mem_usage=True\n",
    "        # , use_cache=False\n",
    "    )\n",
    "    # Quantize\n",
    "    start_time = time.time()\n",
    "    model.quantize(tokenizer, quant_config=quant_config\n",
    "            #    , export_compatible=True\n",
    "               )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Save quantized model\n",
    "    quantized_model_dir = f\"{save_dir}/Llama-3.1-8B-Instruct-AWQ-{w_bit}bit\"\n",
    "    # model.save_quantized(quantized_model_dir)\n",
    "    model.save_quantized(quantized_model_dir, safetensors=False)\n",
    "    tokenizer.save_pretrained(quantized_model_dir)\n",
    "    # model.pack() # makes the model CUDA compat\n",
    "    # model.save_quantized(save_dir + \"/Llama-3.1-8B-Instruct-AWQ-4bit\", safetensors=False)\n",
    "    # tokenizer.save_pretrained(save_dir + \"/Llama-3.1-8B-Instruct-AWQ-4bit\")\n",
    "\n",
    "    # Export to ONNX\n",
    "    # onnx_path = save_dir + \"/Llama-3.1-8B-Instruct-AWQ-4bit/model.onnx\"\n",
    "    # convert_pytorch_to_onnx(model, onnx_path, opset_version=17)\n",
    "    \n",
    "    print(f'Model with w_bit={w_bit} is quantized and saved at \"{quantized_model_dir}\", time: {end_time - start_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note that `shard_checkpoint` is deprecated and will be removed in v4.44. We recommend you using split_torch_state_dict_into_shards from huggingface_hub library\n"
     ]
    }
   ],
   "source": [
    "quantized_model_dir = f\"{save_dir}/Llama-3.1-8B-Instruct-AWQ-{w_bit}bit-tmp\"\n",
    "model.save_quantized(quantized_model_dir, safetensors=True)\n",
    "# tokenizer.save_pretrained(quantized_model_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Green_Halo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
